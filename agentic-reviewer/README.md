---
noteId: "d2b95350cf8b11f08a0769904eccf2a8"
tags: []

---

# Agentic Reviewer

**Semantic Auditing for Text Classification Predictions**

A local-first LLM system that audits text classification predictions through semantic evaluation, alternative suggestions, and natural language explanations. Built with production-oriented patterns and designed for iterative enhancement.

---

## Project Status & Roadmap

> **Current Version:** v1.0 â€” Core Semantic Auditing  
> **In Development:** RAG Edition (Retrieval-Augmented Generation)

This project follows an incremental development philosophy. The table below provides transparency on what's implemented, what's in progress, and what's planned:

| Capability | Status | Notes |
|------------|--------|-------|
| Multi-Task Unified Agent | âœ… **Implemented** | Single LLM call for evaluation, proposal, and reasoning |
| FastAPI REST Interface | âœ… **Implemented** | Full API with health checks, metrics, review endpoints |
| Security Layer | âœ… **Implemented** | Prompt injection detection, input validation, rate limiting |
| LRU Caching + Circuit Breaker | âœ… **Implemented** | Production resilience patterns |
| Audit Logging (SQLite) | âœ… **Implemented** | Complete audit trail for compliance |
| System Monitoring | âœ… **Implemented** | Health checks, memory management, performance metrics |
| RAG: Vector Store | ğŸ”² **Planned** | FAISS/ChromaDB integration (next phase) |
| RAG: Embeddings Service | ğŸ”² **Planned** | sentence-transformers integration |
| RAG: Document Retrieval | ğŸ”² **Planned** | Policy document context injection |
| Evaluation Framework | ğŸ”² **Planned** | Quantitative accuracy metrics |
| CLI Interface | ğŸ”² **Planned** | Interactive terminal experience |

**Why this approach?** Building AI systems requires iterative validation. The core semantic auditing pipeline is functional and demonstrable today. RAG enhancement represents the next evolutionâ€”documented extensively in the implementation plan, with infrastructure designed to support it.

---

## What's Working Today

### Core Semantic Auditing Pipeline

The system evaluates whether a predicted classification label semantically fits the input text, using a three-phase agent approach:

```
Input Text + Predicted Label + Confidence
                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     Unified Agent         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ 1. EVALUATE         â”‚  â”‚  â†’ Correct / Incorrect / Uncertain
    â”‚  â”‚ 2. PROPOSE          â”‚  â”‚  â†’ Alternative label if incorrect
    â”‚  â”‚ 3. REASON           â”‚  â”‚  â†’ Natural language explanation
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
    Verdict + Reasoning + Explanation
```

### Domain: GDPR/CCPA Data Subject Requests

The system is configured for privacy request classification:
- **Access Request** â€” User asks to view their data
- **Erasure** â€” User requests data deletion (right to be forgotten)
- **Rectification** â€” User requests data correction
- **Portability** â€” User requests data export
- **Objection** â€” User objects to processing
- **Complaint** â€” Formal complaint about data handling
- **General Inquiry** â€” Questions about privacy policies

### Production-Oriented Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Layer     â”‚    â”‚  Unified Agent  â”‚    â”‚   LLM Service   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ FastAPI       â”‚â—„â”€â”€â–ºâ”‚ â€¢ Multi-Task    â”‚â—„â”€â”€â–ºâ”‚ â€¢ Ollama        â”‚
â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Circuit       â”‚    â”‚ â€¢ Retry Logic   â”‚
â”‚ â€¢ Rate Limiting â”‚    â”‚   Breaker       â”‚    â”‚ â€¢ Caching       â”‚
â”‚ â€¢ Security      â”‚    â”‚ â€¢ Fallback      â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Layer    â”‚    â”‚  Cache Layer    â”‚    â”‚  Monitoring     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ SQLite        â”‚    â”‚ â€¢ LRU Cache     â”‚    â”‚ â€¢ Health Checks â”‚
â”‚ â€¢ Sample        â”‚    â”‚ â€¢ Persistence   â”‚    â”‚ â€¢ Memory Mgmt   â”‚
â”‚   Selection     â”‚    â”‚ â€¢ TTL Support   â”‚    â”‚ â€¢ Logging       â”‚
â”‚ â€¢ Validation    â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Start

### Prerequisites
- Python 3.11+
- Ollama with Mistral model
- 4GB+ RAM recommended

### Installation

```bash
# Clone repository
git clone https://github.com/your-org/agentic-reviewer.git
cd agentic-reviewer

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### LLM Setup

```bash
# Install Ollama (see https://ollama.ai/download for your platform)
# On Linux/macOS:
curl -fsSL https://ollama.ai/install.sh | sh

# Pull Mistral model
ollama pull mistral

# Start Ollama service (in a separate terminal)
ollama serve
```

### Run the Demo

```bash
# Comprehensive system demonstration
python demo.py
```

The demo script validates the entire pipeline without requiring external services (beyond Ollama), showing:
- Data loading and sample selection
- Agent initialization
- Configuration status
- Cache operations
- (If Ollama running) Live LLM processing

### Start the API Server

```bash
python main.py
```

### Test an API Request

```bash
# Health check
curl http://localhost:8000/health

# Review a prediction
curl -X POST http://localhost:8000/review \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Delete my data permanently",
    "predicted_label": "Access Request",
    "confidence": 0.85
  }'
```

**Expected Response:**
```json
{
  "sample_id": "api_1234567890",
  "verdict": "Incorrect",
  "reasoning": "The text is about deletion, not access",
  "suggested_label": "Erasure",
  "explanation": "This text clearly requests data deletion under the right to be forgotten",
  "success": true,
  "metadata": {
    "model_name": "mistral",
    "tokens_used": 150,
    "latency_ms": 200,
    "agent_type": "unified"
  }
}
```

---

## API Reference

### Endpoints (Implemented)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | System health check with cache and system metrics |
| `GET` | `/metrics` | Detailed performance metrics |
| `GET` | `/stats` | Review statistics and verdict distribution |
| `POST` | `/review` | Review a single prediction |
| `GET` | `/reviews/{verdict}` | Get reviews filtered by verdict |
| `GET` | `/cache/stats` | Cache performance statistics |
| `POST` | `/cache/cleanup` | Clean expired cache entries |
| `GET` | `/security/stats` | Security validation statistics |
| `GET` | `/security/drift` | Drift detection results |

### Authentication

When `AR_API_KEY` is set, endpoints require Bearer token:

```bash
curl -H "Authorization: Bearer your-api-key" http://localhost:8000/stats
```

---

## Configuration

### Environment Variables

```bash
# LLM Configuration
AR_MODEL_NAME=mistral           # Ollama model name
AR_OLLAMA_URL=http://localhost:11434
AR_TEMPERATURE=0.1              # Low for deterministic responses
AR_MAX_TOKENS=512
AR_TIMEOUT=30

# API Configuration
AR_API_HOST=0.0.0.0
AR_API_PORT=8000
AR_API_KEY=your-secure-api-key  # Optional, enables auth
AR_RATE_LIMIT_MAX=100

# Performance Configuration
AR_BATCH_SIZE=10
AR_MAX_CONCURRENT=5
AR_CACHE_MAX_SIZE_MB=100

# Security Configuration
AR_ENABLE_SANITIZATION=true
```

### Custom Labels

Edit `configs/labels.yaml` to adapt to your classification domain:

```yaml
labels:
  - name: "Access Request"
    definition: "User asks to view data held about them under GDPR/CCPA rights."
    examples:
      - "What information do you have about me?"
      - "Send me a copy of my data."
```

### Custom Prompts

Modify prompt templates in `prompts/` to adjust agent behavior:
- `evaluator_prompt.txt` â€” Verdict generation logic
- `proposer_prompt.txt` â€” Alternative label suggestion
- `reasoner_prompt.txt` â€” Explanation generation

---

## Testing

```bash
# Run full test suite
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ --cov=. --cov-report=html

# Run specific test file
python -m pytest tests/test_agents.py -v
```

---

## What's Coming: RAG Edition

The next evolution integrates **Retrieval-Augmented Generation** to enhance reasoning with external knowledge. This is actively planned, with detailed implementation specifications in:

- [`RAG_IMPLEMENTATION_PLAN.md`](RAG_IMPLEMENTATION_PLAN.md) â€” Comprehensive 18-24 week implementation roadmap
- [`RAG_ARCHITECTURE_EVOLUTION.md`](RAG_ARCHITECTURE_EVOLUTION.md) â€” Technical architecture decisions
- [`TECHNICAL_REVIEW_POC_ROADMAP.md`](TECHNICAL_REVIEW_POC_ROADMAP.md) â€” PoC staging and focus points

### RAG Edition Benefits (Planned)

| Enhancement | Impact |
|-------------|--------|
| **Policy Document Retrieval** | Ground reasoning in actual GDPR/CCPA text |
| **Reduced Hallucinations** | External knowledge constrains LLM responses |
| **Citation Support** | Reference specific document sections |
| **Audit Trail Enhancement** | Record which documents informed decisions |

### Planned RAG Stack

```bash
# Future dependencies (not yet required)
faiss-cpu==1.7.4              # Vector database
sentence-transformers==2.2.2  # Local embeddings
```

### Why RAG Isn't Implemented Yet

Building production RAG requires careful foundation work:

1. **Evaluation First** â€” Need baseline metrics before measuring RAG improvements
2. **Infrastructure Stability** â€” Core pipeline must be rock-solid
3. **Ablation Testing** â€” Each RAG component validated independently
4. **No Vendor Lock-in** â€” Local-first approach (FAISS, not cloud vector DBs)

The implementation plan documents this disciplined approach. RAG will be added when the foundation is readyâ€”not before.

---

## Project Structure

```
agentic-reviewer/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py          # LLM abstraction with caching, circuit breaker
â”‚   â”œâ”€â”€ unified_agent.py       # Multi-task processor (single LLM call)
â”‚   â”œâ”€â”€ evaluator.py           # Verdict generation
â”‚   â”œâ”€â”€ proposer.py            # Alternative label suggestion
â”‚   â””â”€â”€ reasoner.py            # Explanation generation
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py              # Hierarchical configuration with validation
â”‚   â”œâ”€â”€ cache.py               # LRU cache with TTL and persistence
â”‚   â”œâ”€â”€ security.py            # Prompt injection & adversarial detection
â”‚   â”œâ”€â”€ monitoring.py          # Health checks and metrics
â”‚   â”œâ”€â”€ review_loop.py         # Batch and single-sample orchestration
â”‚   â”œâ”€â”€ data_loader.py         # CSV data ingestion
â”‚   â”œâ”€â”€ sample_selector.py     # Low-confidence, random, all strategies
â”‚   â””â”€â”€ logger.py              # SQLite audit logging
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ labels.yaml            # Classification label definitions
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ evaluator_prompt.txt   # Jinja2 template for evaluation
â”‚   â”œâ”€â”€ proposer_prompt.txt    # Jinja2 template for proposals
â”‚   â””â”€â”€ reasoner_prompt.txt    # Jinja2 template for reasoning
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.csv              # Sample predictions for testing
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agents.py         # Agent unit tests
â”‚   â””â”€â”€ test_security.py       # Security validation tests
â”œâ”€â”€ demo.py                    # Self-contained demonstration script
â”œâ”€â”€ main.py                    # FastAPI server
â””â”€â”€ requirements.txt           # Python dependencies
```

---

## Design Principles

### Local-First
No cloud dependencies for core functionality. Ollama runs locally, SQLite stores data locally, caching is in-memory. This enables:
- **Privacy** â€” Data never leaves your machine
- **Cost Control** â€” No API usage fees
- **Reliability** â€” No network dependency for inference

### Production Patterns, PoC Scale
The architecture implements enterprise patterns (circuit breakers, caching, security layers) at a scale appropriate for demonstration and iteration. This showcases engineering maturity without over-engineering.

### Explainability by Design
Every classification audit produces:
1. **Verdict** â€” Binary/ternary correctness assessment
2. **Reasoning** â€” Technical justification
3. **Explanation** â€” Stakeholder-friendly narrative

These artifacts support regulatory compliance and human oversight requirements.

### Iterative Enhancement
The codebase is structured for staged improvement:
- Clear separation of concerns enables component replacement
- Configuration-driven behavior enables experimentation
- Modular agents enable A/B testing of approaches

---

## Contributing

```bash
# Development setup
git clone https://github.com/your-org/agentic-reviewer.git
cd agentic-reviewer
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Run tests before committing
python -m pytest tests/ -v
```

### Code Standards
- Follow PEP 8
- Use type hints
- Write docstrings for public methods
- Add tests for new features

---

## License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## Acknowledgments

- **LLM Runtime:** [Ollama](https://ollama.ai/)
- **API Framework:** [FastAPI](https://fastapi.tiangolo.com/)
- **Inspired by:** Research on semantic auditing, LLM agents, and responsible AI practices

---

## Documentation Index

| Document | Purpose |
|----------|---------|
| `README.md` | This file â€” project overview and quick start |
| `TECHNICAL_REVIEW_POC_ROADMAP.md` | Detailed technical review and PoC staging |
| `RAG_IMPLEMENTATION_PLAN.md` | 18-24 week RAG implementation specification |
| `RAG_ARCHITECTURE_EVOLUTION.md` | Technical architecture for RAG integration |
| `RAG_IMPLEMENTATION_CHECKLIST.md` | Task checklist for RAG development |

---

*This project demonstrates AI engineering practices including LLM integration, production resilience patterns, security awareness, and iterative development methodology. The documentation reflects the actual state of implementationâ€”what works today and what's planned for tomorrow.*
